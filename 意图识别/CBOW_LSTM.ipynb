{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 需要注意一个重点：pad处理时需要把test和train的长度pad到一致"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import jieba\n",
    "from torch.nn.utils.rnn import pad_sequence,pad_packed_sequence,pack_sequence,pack_padded_sequence\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"/data/linqinhong/kaggle/yitushibie/data/yitushibie/train.csv\",header=None,names=[\"text\",\"title\"],sep=\"\\t\")\n",
    "test_data=pd.read_csv(\"/data/linqinhong/kaggle/yitushibie/data/yitushibie/test.csv\",header=None,names=[\"text\",\"title\"],sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.601 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_data[\"process\"]=train_data[\"text\"].apply(jieba.lcut)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "test_data[\"process\"]=test_data[\"text\"].apply(jieba.lcut)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword=stopwords.words(\"chinese\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_data[\"process\"]=train_data[\"process\"].apply(lambda x: [i for i in x if i not in stopword])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "test_data[\"process\"]=test_data[\"process\"].apply(lambda x: [i for i in x if i not in stopword])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "process=list(train_data[\"process\"])+list(test_data[\"process\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "CBOW=Word2Vec(sentences=process,sg=0,min_count=1,window=4,vector_size=300,epochs=300,workers=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_num(text):\n",
    "    total_num = []\n",
    "    if text == []:\n",
    "        return torch.tensor([])\n",
    "    for word in text:\n",
    "        index = CBOW.wv.key_to_index[word]\n",
    "        total_num.append(index)\n",
    "    return torch.tensor(total_num,dtype=torch.int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_vec(text):\n",
    "    total_vec=[]\n",
    "    if text==[]:\n",
    "        return None\n",
    "    for word in text:\n",
    "        index=CBOW.wv.key_to_index[word]\n",
    "        vec=np.reshape(CBOW.wv[index],(1,300))\n",
    "        #vec=(CBOW.wv[index])\n",
    "        total_vec.append(vec)\n",
    "    return np.concatenate(total_vec,axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_data[\"num\"]=train_data[\"process\"].apply(get_num)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "test_data[\"num\"]=test_data[\"process\"].apply(get_num)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "pad_num=len(CBOW.wv.key_to_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "num_list=list(train_data[\"num\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "test_num_list=list(test_data[\"num\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([   91,  2683,  3476, 13067, 13067, 13067, 13067, 13067, 13067, 13067,\n        13067, 13067, 13067, 13067, 13067, 13067, 13067, 13067, 13067, 13067,\n        13067, 13067], dtype=torch.int32)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_num_list[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "padded_numtensor=pad_sequence(num_list,padding_value=pad_num,batch_first=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "test_pad_num=pad_sequence(test_num_list,padding_value=pad_num,batch_first=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "22"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pad_num[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "padded_numlist=[i.tolist() for i in padded_numtensor]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "test_data[\"padded_num\"]=[i.tolist() for i in test_pad_num]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "train_data[\"padded_num\"]=padded_numlist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "train_data.drop(\"num\",inplace=True,axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(13067, 300)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBOW.wv.vectors.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "'embedding=np.concatenate((CBOW.wv.vectors,np.reshape(np.array([0.0] * 300),(1,300))),axis=0)'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding=torch.tensor(np.concatenate((CBOW.wv.vectors,np.reshape(np.array([0.0] * 300),(1,300))),axis=0),dtype=torch.double)\n",
    "\"\"\"embedding=np.concatenate((CBOW.wv.vectors,np.reshape(np.array([0.0] * 300),(1,300))),axis=0)\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'def get_tensor(text):\\n    nplist=[]\\n    for i in text:\\n        nplist.append(np.reshape(embedding[i],(1,-1)))\\n    return np.concatenate(nplist,axis=1)'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tensor(text):\n",
    "    tensor=[]\n",
    "    for i in text:\n",
    "        tensor.append(torch.reshape(embedding[i],(1,-1)))\n",
    "    return torch.cat(tensor,dim=0)\n",
    "\"\"\"def get_tensor(text):\n",
    "    nplist=[]\n",
    "    for i in text:\n",
    "        nplist.append(np.reshape(embedding[i],(1,-1)))\n",
    "    return np.concatenate(nplist,axis=1)\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "train_data[\"vec\"]=train_data[\"padded_num\"].apply(get_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\"\"\"class mydataset(Dataset):\n",
    "    def __init__(self,data,label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.len = len(self.label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\"\"\"\n",
    "#train_set=Dataset(train_data[[\"vec\",\"label\"]])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "labelsset=set(train_data[\"title\"])\n",
    "labelsdict={}\n",
    "reverser_dict={}\n",
    "for index,keys in enumerate(labelsset):\n",
    "    labelsdict[keys]=index\n",
    "for index,keys in enumerate(labelsset):\n",
    "    reverser_dict[index]=keys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def convert(text):\n",
    "    return labelsdict[text]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "train_data[\"label\"]=train_data[\"title\"].apply(convert)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "train_data.drop([\"text\",\"title\",\"process\"],axis=1,inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:1\") if torch.cuda.is_available()==True else torch.device(\"cpu\")\n",
    "from torch import relu\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,input_channels,hidden_channels,num_layers,output_channels,embedding):\n",
    "        super(LSTMModule,self).__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.input=input_channels\n",
    "        self.hidden=hidden_channels\n",
    "        self.output=output_channels\n",
    "        self.lstm=nn.LSTM(input_channels,hidden_channels,num_layers,batch_first=True,dropout=0.3)\n",
    "        self.fc=nn.Linear(hidden_channels,hidden_channels)\n",
    "        self.fc1=nn.Linear(hidden_channels,output_channels)\n",
    "        #self.Embed=nn.Linear(embedding.shape[0],embedding.shape[1],)\n",
    "        self.Embed=nn.Embedding(embedding.shape[0],embedding.shape[1],_weight=embedding)\n",
    "        for para in self.Embed.parameters():\n",
    "           para.requires_grad=False\n",
    "        self.drop=nn.Dropout(0.5)\n",
    "        self.batch=nn.modules.batchnorm.BatchNorm1d(22)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.Embed(x).float()\n",
    "        batch_size,seq_len=x.shape[0],x.shape[1]\n",
    "        h0=torch.zeros(self.num_layers,batch_size,self.hidden,dtype=torch.float,requires_grad=True).to(device)\n",
    "        c0=torch.zeros(self.num_layers,batch_size,self.hidden,dtype=torch.float,requires_grad=True).to(device)\n",
    "        out,(hn,cn)=self.lstm(x,(h0,c0))\n",
    "        \"\"\"out=self.fc1(out)[:,-1,:]\"\"\"\n",
    "        out=self.fc(out)\n",
    "        #print(out.shape)\n",
    "        out=relu(out)\n",
    "        self.batch(out)\n",
    "        self.drop(out)\n",
    "        #print(out.shape)\n",
    "        out=self.fc1(out)\n",
    "        #print(out.shape)\n",
    "        out=torch.mean(out,dim=1)\n",
    "        #print(out.shape)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "class num_dataset(Dataset):\n",
    "    def __init__(self,numlist,label):\n",
    "        self.len=len(label)\n",
    "        self.num=numlist\n",
    "        self.label=label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.num[index],self.label[index]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "train_data[\"padded_tensor\"]=train_data[\"padded_num\"].apply(lambda x: torch.tensor(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "test_data[\"padded_tensor\"]=test_data[\"padded_num\"].apply(lambda x: torch.tensor(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "LSTM_class=LSTMModule(300,300,3,12,embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6662269129287599\n",
      "loss: 0.9278104120477209\n",
      "acc: 0.8771437994722955\n",
      "loss: 0.4325538180856953\n",
      "acc: 0.9046009234828496\n",
      "loss: 0.32612642608730413\n",
      "acc: 0.9234828496042217\n",
      "loss: 0.26954858418255967\n",
      "acc: 0.9296668865435356\n",
      "loss: 0.23486853080573136\n",
      "acc: 0.9444261213720316\n",
      "loss: 0.19158058010989623\n",
      "acc: 0.9492908970976254\n",
      "loss: 0.16635953769088213\n",
      "acc: 0.9572889182058048\n",
      "loss: 0.14499924529475464\n",
      "acc: 0.9608344327176781\n",
      "loss: 0.1248739889810153\n",
      "acc: 0.9656992084432717\n",
      "loss: 0.10952890098779504\n",
      "acc: 0.9686675461741425\n",
      "loss: 0.09619220312297594\n",
      "acc: 0.9731200527704486\n",
      "loss: 0.08354377154454065\n",
      "acc: 0.9769129287598944\n",
      "loss: 0.07405599408510694\n",
      "acc: 0.9781497361477572\n",
      "loss: 0.07102931905709099\n",
      "acc: 0.983344327176781\n",
      "loss: 0.06267008564963551\n",
      "acc: 0.9838390501319261\n",
      "loss: 0.05200804310802632\n",
      "acc: 0.9860653034300791\n",
      "loss: 0.05653022466199014\n",
      "acc: 0.9881266490765171\n",
      "loss: 0.04085964136463096\n",
      "acc: 0.9890336411609498\n",
      "loss: 0.0415122119868693\n",
      "acc: 0.9897757255936676\n",
      "loss: 0.0399243076125363\n",
      "acc: 0.9905178100263852\n",
      "loss: 0.03260134621357721\n",
      "acc: 0.9924967018469657\n",
      "loss: 0.03009357285323602\n",
      "acc: 0.9920019788918206\n",
      "loss: 0.029923820893740384\n",
      "acc: 0.9926616094986808\n",
      "loss: 0.029548797911037115\n",
      "acc: 0.9923317941952506\n",
      "loss: 0.026061092456655225\n",
      "acc: 0.9943106860158312\n",
      "loss: 0.029828977725650713\n",
      "acc: 0.9952176781002638\n",
      "loss: 0.014577922220516007\n",
      "acc: 0.9949703166226913\n",
      "loss: 0.01887315802409651\n",
      "acc: 0.9951352242744064\n",
      "loss: 0.022314989677304112\n",
      "acc: 0.9954650395778364\n",
      "loss: 0.016990014359917073\n",
      "acc: 0.9953825857519789\n",
      "loss: 0.015232988459241023\n",
      "acc: 0.996042216358839\n",
      "loss: 0.013196438211408697\n",
      "acc: 0.9948878627968337\n",
      "loss: 0.021548940283599806\n",
      "acc: 0.996042216358839\n",
      "loss: 0.015995640735945785\n",
      "acc: 0.9961246701846965\n",
      "loss: 0.014022578213850613\n",
      "acc: 0.9959597625329816\n",
      "loss: 0.012953651597529203\n",
      "acc: 0.9966193931398417\n",
      "loss: 0.012788193299462137\n",
      "acc: 0.9963720316622692\n",
      "loss: 0.011475157890242903\n",
      "acc: 0.9962071240105541\n",
      "loss: 0.01637020593238729\n",
      "acc: 0.9962071240105541\n",
      "loss: 0.015409794264922484\n",
      "acc: 0.9965369393139841\n",
      "loss: 0.015675237146038797\n",
      "acc: 0.9971141160949868\n",
      "loss: 0.009149464713108447\n",
      "acc: 0.997608839050132\n",
      "loss: 0.007726907593575168\n",
      "acc: 0.9971965699208444\n",
      "loss: 0.014444483568676064\n",
      "acc: 0.997608839050132\n",
      "loss: 0.010471255407964574\n",
      "acc: 0.9969492084432717\n",
      "loss: 0.011593408637069721\n",
      "acc: 0.9971141160949868\n",
      "loss: 0.011998855542551704\n",
      "acc: 0.9974439313984169\n",
      "loss: 0.010802212589988397\n",
      "acc: 0.9971141160949868\n",
      "loss: 0.0170954645183397\n",
      "acc: 0.9972790237467019\n",
      "loss: 0.012140279513622624\n",
      "acc: 0.9974439313984169\n",
      "loss: 0.008799038266510555\n",
      "acc: 0.9976912928759895\n",
      "loss: 0.010492390484148103\n",
      "acc: 0.997608839050132\n",
      "loss: 0.011544331156858966\n",
      "acc: 0.9977737467018469\n",
      "loss: 0.006542135299057373\n",
      "acc: 0.9978562005277045\n",
      "loss: 0.009315884529074035\n",
      "acc: 0.9984333773087071\n",
      "loss: 0.006256418848655615\n",
      "acc: 0.9981035620052771\n",
      "loss: 0.005816125897197815\n",
      "acc: 0.9984333773087071\n",
      "loss: 0.005737966401220218\n",
      "acc: 0.9978562005277045\n",
      "loss: 0.007387343700972802\n",
      "acc: 0.9981860158311345\n",
      "loss: 0.0065162281096525065\n",
      "acc: 0.997608839050132\n",
      "loss: 0.011870649496809117\n",
      "acc: 0.9989281002638523\n",
      "loss: 0.0032841928445881442\n",
      "acc: 0.9985982849604221\n",
      "loss: 0.005543453573418444\n",
      "acc: 0.9989281002638523\n",
      "loss: 0.007428882853111039\n",
      "acc: 0.9986807387862797\n",
      "loss: 0.0056140210512040595\n",
      "acc: 0.9984333773087071\n",
      "loss: 0.007824963569806815\n",
      "acc: 0.9985158311345647\n",
      "loss: 0.0043233565604310666\n",
      "acc: 0.9989281002638523\n",
      "loss: 0.0050366842293281715\n",
      "acc: 0.9989281002638523\n",
      "loss: 0.003603845621307112\n",
      "acc: 0.9985158311345647\n",
      "loss: 0.006929026827220946\n",
      "acc: 0.9991754617414248\n",
      "loss: 0.0022342618051587404\n",
      "acc: 0.9985158311345647\n",
      "loss: 0.007511914803020348\n",
      "acc: 0.9985158311345647\n",
      "loss: 0.005100341012105208\n",
      "acc: 0.9983509234828496\n",
      "loss: 0.007879800461638088\n",
      "acc: 0.9989281002638523\n",
      "loss: 0.0032283814820942415\n",
      "acc: 0.9988456464379947\n",
      "loss: 0.006036090684582476\n",
      "acc: 0.9988456464379947\n",
      "loss: 0.004365329331469303\n",
      "acc: 0.9991754617414248\n",
      "loss: 0.0025417387853662477\n",
      "acc: 0.9990105540897097\n",
      "loss: 0.002923323596707415\n",
      "acc: 0.9989281002638523\n",
      "loss: 0.004722106270945624\n",
      "acc: 0.9987631926121372\n",
      "loss: 0.005842692379416712\n",
      "acc: 0.9990105540897097\n",
      "loss: 0.004110379623543\n",
      "acc: 0.9992579155672823\n",
      "loss: 0.0027849510405097973\n",
      "acc: 0.9989281002638523\n",
      "loss: 0.0027931388656932434\n",
      "acc: 0.9990930079155673\n",
      "loss: 0.0056756536043487595\n",
      "acc: 0.9991754617414248\n",
      "loss: 0.002277512249920115\n",
      "acc: 0.9990105540897097\n",
      "loss: 0.005723764396095829\n",
      "acc: 0.9991754617414248\n",
      "loss: 0.003950543008090831\n",
      "acc: 0.9987631926121372\n",
      "loss: 0.006048532283921662\n",
      "acc: 0.9990930079155673\n",
      "loss: 0.002254062062575121\n",
      "acc: 0.9990105540897097\n",
      "loss: 0.0025934729819404394\n",
      "acc: 0.9990930079155673\n",
      "loss: 0.005261465491179945\n",
      "acc: 0.9988456464379947\n",
      "loss: 0.005462176560342606\n",
      "acc: 0.9992579155672823\n",
      "loss: 0.001664480708209558\n",
      "acc: 0.9990105540897097\n",
      "loss: 0.004631979759704347\n",
      "acc: 0.9989281002638523\n",
      "loss: 0.00538246451449481\n",
      "acc: 0.9987631926121372\n",
      "loss: 0.003629441396230639\n",
      "acc: 0.9991754617414248\n",
      "loss: 0.002404795909520329\n",
      "acc: 0.9990930079155673\n",
      "loss: 0.003015370841238992\n",
      "acc: 0.9993403693931399\n",
      "loss: 0.0030164820491761324\n"
     ]
    }
   ],
   "source": [
    "train_set=num_dataset(train_data[\"padded_tensor\"],train_data[\"label\"])\n",
    "train_loader=DataLoader(train_set,shuffle=True,batch_size=32)\n",
    "\"\"\"train_set=num_dataset(train_data[\"padded_num\",train_data[\"label\"]])\n",
    "train_loader=DataLoader(train_set,shuffle=True,batch_size=32)\"\"\"\n",
    "loss_fn=CrossEntropyLoss()\n",
    "epoches=100\n",
    "total_loss=[]\n",
    "total_acc=[]\n",
    "num_step=epoches*len(train_loader)\n",
    "import evaluate\n",
    "from torch.optim.adam import Adam\n",
    "metric=evaluate.load(\"accuracy\")\n",
    "#optimizer=Adam(LSTM_class.parameters(),lr=0.05)\n",
    "#lr_scheduler=get_scheduler('linear',optimizer=optimizer,num_warmup_steps=num_step*0.2)\n",
    "device=torch.device(\"cuda:1\") if torch.cuda.is_available()==True else torch.device(\"cpu\")\n",
    "LSTM_class.to(device)\n",
    "for epoch in range(epoches):\n",
    "    loss_num=0\n",
    "    acc=0\n",
    "    optimizer=Adam(LSTM_class.parameters(),lr=0.005/(epoch/10+1))\n",
    "    for i,(data,label) in enumerate(train_loader):\n",
    "        #out=LSTM_class(data.float())\n",
    "        #data.to(device)\n",
    "        #label.to(device)\n",
    "        out=LSTM_class(data.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss=loss_fn(out,label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #lr_scheduler.step()\n",
    "        loss_num+=loss.item()\n",
    "        metric.add_batch(predictions=torch.argmax(out,dim=1),references=label)\n",
    "        acc+=metric.compute()[\"accuracy\"]\n",
    "    print(\"acc:\",acc/len(train_loader))\n",
    "    loss_num/=len(train_loader)\n",
    "    total_loss.append(loss_num)\n",
    "    print(\"loss:\",loss_num)\n",
    "    total_acc.append(acc/len(train_loader))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "class test_dataset(Dataset):\n",
    "    def __init__(self,numlist):\n",
    "        self.len=len(numlist)\n",
    "        self.num=numlist\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.num[index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "test_set=test_dataset(test_data[\"padded_tensor\"])\n",
    "test_loader=DataLoader(test_set,shuffle=True,batch_size=32)\n",
    "\"\"\"train_set=num_dataset(train_data[\"padded_num\",train_data[\"label\"]])\n",
    "train_loader=DataLoader(train_set,shuffle=True,batch_size=32)\"\"\"\n",
    "loss_fn=CrossEntropyLoss()\n",
    "epoches=100\n",
    "total_loss=[]\n",
    "total_acc=[]\n",
    "num_step=epoches*len(train_loader)\n",
    "import evaluate\n",
    "from torch.optim.adam import Adam\n",
    "metric=evaluate.load(\"accuracy\")\n",
    "#optimizer=Adam(LSTM_class.parameters(),lr=0.05)\n",
    "#lr_scheduler=get_scheduler('linear',optimizer=optimizer,num_warmup_steps=num_step*0.2)\n",
    "device=torch.device(\"cuda:1\") if torch.cuda.is_available()==True else torch.device(\"cpu\")\n",
    "LSTM_class.to(device)\n",
    "total_pred=[]\n",
    "for i,(data) in enumerate(test_loader):\n",
    "    #out=LSTM_class(data.float())\n",
    "    #data.to(device)\n",
    "    #label.to(device)\n",
    "    out=LSTM_class(data.to(device))\n",
    "    pred=torch.argmax(out,dim=1)\n",
    "    total_pred.extend(pred.tolist())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "3000"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "test_rawdata=pd.read_csv(\"/data/linqinhong/kaggle/yitushibie/data/yitushibie/test.csv\",header=None,names=[\"text\",\"title\"],sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "title=[reverser_dict[i] for i in total_pred]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "test_rawdata[\"title\"]=title"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "test_rawdata.to_csv(\"/data/linqinhong/kaggle/yitushibie/data/yitushibie/result_CBOW_LSTM.csv\",index=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
